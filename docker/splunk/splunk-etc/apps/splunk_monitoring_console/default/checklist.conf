[orphaned_scheduled_searches]
title = Orphaned scheduled searches
category = Splunk Miscellaneous
tags = configuration, search, searches_skipped
description = This looks for scheduled searches that have become orphaned after their owner was deactivated. Typically, a search is orphaned when a user is removed from the external authentication system (LDAP or AD) that Splunk software relies on.
failure_text = One or more scheduled searches are orphaned, meaning that they are no longer associated with valid owners. The scheduler will not run orphaned scheduled searches.
suggested_action = Identify the impacted searches by using the provided drill-down search or by consulting the "Orphaned Scheduled Searches" dashboard on reported instances. Disable, delete, or reassign ownership of the affected searches, as appropriate.
doc_link = learnmore.orphaned_searches
doc_title = orphaned searches
applicable_to_groups = dmc_group_search_head
search = | rest $rest_scope$ /servicesNS/-/-/saved/searches add_orphan_field=yes count=0 \
| where disabled=0 AND is_scheduled=1 \
| stats count(eval(orphan=1)) AS orphaned_search_count by splunk_server \
| eval severity_level = if(orphaned_search_count > 0, 3, 0) \
| rename splunk_server AS instance
drilldown = | rest splunk_server=$instance$ /servicesNS/-/-/saved/searches add_orphan_field=yes count=0 \
| search orphan=1 disabled=0 is_scheduled=1 \
| eval status = if(disabled = 0, "enabled", "disabled") \
| fields title eai:acl.owner eai:acl.app eai:acl.sharing orphan status is_scheduled cron_schedule next_scheduled_time next_scheduled_time actions \
| rename title AS "search name" eai:acl.owner AS owner eai:acl.app AS app eai:acl.sharing AS sharing

[search_scheduler_skip_ratio]
title = Search scheduler skip ratio
category = Data Search
tags = scheduler, searches_skipped
description = This checks whether scheduled searches were skipped in the past hour.
failure_text = Scheduled searches are being skipped on one or more search heads.
suggested_action = Check the Monitoring Console Scheduler Activity: Instance dashboard for each affected search head to determine the problem.
doc_link = healthcheck.scheduler.skip
doc_title = skipping scheduled searches
applicable_to_groups = dmc_group_search_head
search = earliest=-60m `dmc_set_index_internal` search_group=dmc_group_search_head sourcetype=scheduler (status="completed" OR status="skipped") \
| stats count(eval(status=="completed" OR status=="skipped")) AS total_executions, count(eval(status=="skipped")) AS skipped_executions by host \
| eval skip_ratio = skipped_executions/total_executions * 100 \
| eval severity_level = case(skip_ratio == 0, 0, skip_ratio > 0, 2) \
| eval skip_ratio = round(skip_ratio, 2)."%" \
| rename host AS instance \
| fields instance total_executions skipped_executions skip_ratio severity_level
drilldown = /app/splunk_monitoring_console/scheduler_activity_instance?form.splunk_server=$instance$

[missing_forwarders]
title = Missing forwarders
category = Data Indexing
tags = forwarding, batchreader, tailreader
description = This checks whether any forwarders have not connected to indexers for >15 minutes in the recent past. \
NOTE: This check requires the Monitoring Console forwarder monitoring feature to be enabled.
failure_text = One or more forwarders that previously communicated with your indexers have not been seen for more than 15 minutes.
suggested_action = Check the Forwarders: Deployment dashboard to get a full list of the impacted forwarders.
doc_link = healthcheck.forwarders.setup, healthcheck.forwarders.troubleshoot
doc_title = forwarder setup, forwarder troubleshooting
applicable_to_groups = dmc_group_indexer
search = | inputlookup dmc_forwarder_assets \
| stats count AS forwarder_count by status \
| eval severity_level = case(status=="active", 0, status=="missing", 2, true(), -1) \
| eventstats max(severity_level) AS max_severity_level \
| where severity_level = max_severity_level \
| fields status forwarder_count severity_level
drilldown = /app/splunk_monitoring_console/forwarder_deployment

[expiring_or_expired_licenses]
title = Expiring or expired licenses
category = Data Indexing
tags = licensing
description = This checks for licenses that are about to expire or have already expired.
failure_text = One or more licenses installed on your license manager(s) have expired or will expire soon.
suggested_action = Contact your sales representative if you need a new license. Make sure to delete already expired licenses on the license manager page.
doc_link = healthcheck.expired.license
doc_title = expired licenses
applicable_to_groups = dmc_group_license_master
search = | rest $rest_scope$ splunk_server_group=dmc_group_license_master /services/licenser/licenses \
| join type=outer group_id splunk_server [ \
    rest $rest_scope$ splunk_server_group=dmc_group_license_master /services/licenser/groups \
    | where is_active = 1 \
    | rename title AS group_id \
    | fields is_active group_id splunk_server] \
| where is_active = 1 \
| eval days_left = floor((expiration_time - now()) / 86400) \
| where NOT (quota = 1048576 OR label == "Splunk Enterprise Reset Warnings" OR label == "Splunk Lite Reset Warnings") \
| eval status = case(days_left >= 14, days_left." days left", days_left < 14 AND days_left >= 0, "Expires soon: ".days_left." days left", days_left < 0, "Expired") \
| eval size_gb = round(quota/1024/1024/1024,3) \
| fields splunk_server label license_hash size_gb days_left status \
| eval license_info = label." (size = ".size_gb." GB ; hash = ".license_hash.") - ".status \
| eval severity_level = case(days_left >= 14, 0, days_left < 14 AND days_left >= 0, 2, days_left < 0, 3) \
| stats values(license_info) AS license_information max(severity_level) AS severity_level by splunk_server \
| rename splunk_server AS instance

[license_warnings_and_violations]
title = License warnings and violations
category  = Data Indexing
tags = indexing, licensing
description = This checks whether any license peers have incurred license warnings in the current licensing window and whether any are in violation.
failure_text = One or more license peers have warnings or are in violation. An indexer in violation is not capable of serving searches.
suggested_action = Check the Monitoring Console license usage dashboards to understand how warnings were accrued. \
To clear violations, request a reset license from Splunk Support.
doc_link = healthcheck.license.warnings.violation
doc_title = license warnings and violations
applicable_to_groups = dmc_group_license_master
search = | rest $rest_scope$ splunk_server_group=dmc_group_license_master /services/licenser/slaves \
| join type=outer splunk_server [rest $rest_scope$ /services/server/info | fields version, splunk_server] \
| fields label splunk_server version warning_count \
| eval in_violation = if(warning_count >= 5, 1, 0) \
| eval severity_level = case(in_violation == 1, 3, warning_count > 0 AND warning_count < 5, 2, warning_count == 0, 0) \
| rename label AS instance splunk_server AS license_master version AS "license_master version" warning_count AS "warning_count (current / limit)" \
| fieldformat warning_count (current / limit) = 'warning_count (current / limit)'." / 5" \
| fields - _timediff
drilldown = /app/splunk_monitoring_console/license_usage_today?form.splunk_server=$instance$

[system_hardware_provisioning_assessment]
title = System hardware provisioning assessment
category = System and Environment
tags = best_practices, capacity, scalability
description = This evaluates the hardware specifications of Splunk instances tasked with indexing and/or searching data, using reference hardware.
failure_text = One or more hosts has returned CPU or memory specifications that fall below reference hardware recommendations. This might adversely affect indexing or search performance.
suggested_action = If you are experiencing performance issues, consider upgrading hosts to meet or exceed the recommended hardware specs.
doc_link = healthcheck.hardware.reference
doc_title = recommended minimum hardware
applicable_to_groups = dmc_group_search_head,dmc_group_indexer
search = | rest $rest_scope$ /services/server/info \
| eval cpu_core_count = if(isnotnull(numberOfVirtualCores), numberOfVirtualCores, numberOfCores) \
| eval physical_memory_GB = round(physicalMemoryMB / 1024, 0) \
| fields splunk_server cpu_core_count physical_memory_GB \
| eval severity_level = case(cpu_core_count <= 4 OR physical_memory_GB <= 4, 2, cpu_core_count < 12 OR physical_memory_GB < 12, 1, cpu_core_count >= 12 AND physical_memory_GB >= 12, 0, true(), -1) \
| rename splunk_server AS instance cpu_core_count AS "cpu_core_count (current / recommended)" physical_memory_GB AS "physical_memory_GB (current / recommended)" \
| fieldformat cpu_core_count (current / recommended) = 'cpu_core_count (current / recommended)'." / 12" \
| fieldformat physical_memory_GB (current / recommended) = 'physical_memory_GB (current / recommended)'." / 12"

[linux_kernel_transparent_huge_pages]
title = Linux kernel transparent huge pages
category = System and Environment
tags = best_practices, operating_system
description = This attempts to determine whether Splunk is running on a Linux server where kernel transparent huge pages are enabled. NOTE: This check is relevant only for Linux. NOTE: This check yields results only for instances that are running Splunk Enterprise 6.5 or higher. Instances running an older version produce search errors that can be ignored.
failure_text = One or more Splunk instances are running on a host that has kernel transparent huge pages enabled. This can significantly reduce performance and is against best practice.
suggested_action = Turn off kernel transparent huge pages.
doc_link = healthcheck.hardware.thp
doc_title = transparent huge pages
applicable_to_groups =
search = | rest $rest_scope$ /services/server/info \
| join type=outer splunk_server [rest $rest_scope$ /services/server/sysinfo | fields splunk_server transparent_hugepages.*] \
| eval transparent_hugepages.effective_state = if(isnotnull('transparent_hugepages.effective_state'), 'transparent_hugepages.effective_state', "unknown") \
| eval transparent_hugepages.enabled = case(len('transparent_hugepages.enabled') > 0, 'transparent_hugepages.enabled', 'transparent_hugepages.effective_state' == "ok" AND (isnull('transparent_hugepages.enabled') OR len('transparent_hugepages.enabled') = 0), "feature not available", 'transparent_hugepages.effective_state' == "unknown" AND isnull('transparent_hugepages.enabled'), "unknown", True(), "unknown") \
| eval transparent_hugepages.defrag = case(len('transparent_hugepages.defrag') > 0, 'transparent_hugepages.defrag', 'transparent_hugepages.effective_state' == "ok" AND (isnull('transparent_hugepages.defrag') OR len('transparent_hugepages.defrag') = 0), "feature not available", 'transparent_hugepages.effective_state' == "unknown" AND isnull('transparent_hugepages.defrag'), "unknown", True(), "unknown") \
| eval severity_level = case('transparent_hugepages.effective_state' == "unavailable", -1, 'transparent_hugepages.effective_state' == "ok", 0, 'transparent_hugepages.effective_state' == "unknown", 1, 'transparent_hugepages.effective_state' == "bad", 2) \
| fields splunk_server transparent_hugepages.enabled transparent_hugepages.defrag transparent_hugepages.effective_state severity_level \
| rename splunk_server AS instance \
| fields - _timediff

[assessment_of_server_ulimits]
title = Assessment of server ulimits
category = System and Environment
tags = best_practices, operating_system
description = This checks whether the machine is provisioned with ulimit settings (file descriptors, user processes, and data segment size) that are adequate for running Splunk software.
failure_text = One or more Splunk instances are running on a host that has one or more resource limits set below official recommendations.
suggested_action = Persistently modify resource limits per documented best practices.
doc_link = healthcheck.software.ulimit
doc_title = Splunk software ulimit requirements
applicable_to_groups =
search = | rest $rest_scope$ /services/server/info \
| join type=outer splunk_server [rest $rest_scope$ /services/server/sysinfo | fields splunk_server ulimits.data_segment_size ulimits.open_files ulimits.user_processes] \
| eval ulimits.data_segment_size = if(isnotnull('ulimits.data_segment_size'), 'ulimits.data_segment_size', "unavailable") \
| eval ulimits.open_files = if(isnotnull('ulimits.open_files'), 'ulimits.open_files', "unavailable") \
| eval ulimits.user_processes = if(isnotnull('ulimits.user_processes'), 'ulimits.user_processes', "unavailable") \
| eval sev_segment_size = case('ulimits.data_segment_size' == -1 OR 'ulimits.data_segment_size' >= 1073741824, 0, 'ulimits.data_segment_size' == "unavailable", -1, True(), 2) \
| eval sev_open_files = case('ulimits.open_files' == -1 OR 'ulimits.open_files' >= 64000, 0, 'ulimits.open_files' == "unavailable", -1, True(), 2) \
| eval sev_user_processes = case('ulimits.user_processes' == -1 OR 'ulimits.user_processes' >= 16000, 0, 'ulimits.user_processes' == "unavailable", -1, True(), 2) \
| eval severity_level = max(sev_segment_size, sev_open_files, sev_user_processes) \
| fields splunk_server ulimits.data_segment_size ulimits.open_files ulimits.user_processes severity_level \
| rename splunk_server AS instance ulimits.data_segment_size AS "ulimits.data_segment_size (current / recommended)" ulimits.open_files AS "ulimits.open_files (current / recommended)" ulimits.user_processes AS "ulimits.user_processes (current / recommended)" \
| fieldformat ulimits.data_segment_size (current / recommended) = (if('ulimits.data_segment_size (current / recommended)' >= 0, 'ulimits.data_segment_size (current / recommended)', 'ulimits.data_segment_size (current / recommended)'))." / 1073741824" \
| fieldformat ulimits.open_files (current / recommended) = (if('ulimits.open_files (current / recommended)' >= 0, 'ulimits.open_files (current / recommended)', 'ulimits.open_files (current / recommended)'))." / 64000" \
| fieldformat ulimits.user_processes (current / recommended) = (if('ulimits.user_processes (current / recommended)' >= 0, 'ulimits.user_processes (current / recommended)', 'ulimits.user_processes (current / recommended)'))." / 16000" \
| fields - _timediff

[SHP_to_SHC_upgrade_opportunity]
title = Upgrade opportunity from search head pooling to search head clustering
category = Splunk Miscellaneous
tags = best_practices, configuration
description = If you are using search head pooling and running Splunk Enterprise 6.2 or higher, we strongly recommend that you migrate to search head clustering, a feature designed to replace search head pooling that removes the dependency on shared storage and offers other advantages.
failure_text = This instance is part of a search head pool and is running Splunk Enterprise 6.2 or higher.
suggested_action = Consult the procedure to migrate from search head pooling to search head clustering
doc_link = healthcheck.shp.upgradenag
doc_title = upgrading from search head pooling
applicable_to_groups = dmc_group_search_head
environments_to_exclude = standalone
search = | rest $rest_scope$ /services/properties/server/pooling/state \
| where value=enabled \
| eval shpooling = 1 \
| join type=outer splunk_server [rest $rest_scope$ /services/server/info | fields version, splunk_server] \
| stats max(shpooling) AS shpooling_state max(version) AS version by splunk_server \
| fields splunk_server version shpooling_state \
| eval severity_level = if(shpooling_state = 1 AND match(version,"^(6\.[234]|[7-9]\.)"), 1, 0) \
| rename splunk_server AS instance \
| fields - _timediff

[distributed_search_health_assessment]
title = Distributed search health assessment
category = Data Search
tags = distributed_search, indexers
description = This checks the status of the search peers (indexers) of each search head. It assesses the overall health of distributed search on a per search head basis.
failure_text = One or more search heads are reporting unhealthy search peers.
suggested_action = Check the Monitoring Console Distributed Search: Instance view for more information. Review resource usage and general instance health of the unhealthy peers.
doc_link = healthcheck.distributedsearch
doc_title = Splunk distributed search
applicable_to_groups = dmc_group_search_head
environments_to_exclude = standalone
search = | rest $rest_scope$ splunk_server_group=dmc_group_search_head /services/search/distributed/peers \
| fields splunk_server peerName server_roles status status_details \
| eval peer_health = case(status == "Up" AND isnull(status_details), "optimal", status != "Down" AND isnotnull(status_details), "degraded", status == "Down" , "down", status!= "Up" OR status != "Down", "inconclusive") \
| stats count AS peer_count count(eval(peer_health == "optimal")) AS peer_count_optimal count(eval(peer_health == "degraded")) AS peer_count_degraded,  count(eval(peer_health == "down")) AS peer_count_down, count(eval(peer_health == "inconclusive")) AS peer_count_inconclusive, by splunk_server \
| eval severity_level = case(peer_count_down > 0, 3, peer_count_degraded > 0, 2, peer_count_inconclusive > 0, 1, peer_count_optimal > 0, 0, True(), -1) \
| rename splunk_server AS search_head
drilldown = /app/splunk_monitoring_console/distributed_search_instance?form.splunk_server=$search_head$

[local_indexing_on_non-indexer_instances]
title = Local indexing on non-indexer instances
category = Data Indexing
tags = best_practices, forwarding, indexing
description = This checks whether any non-indexer instances (search heads, indexer cluster manager nodes, license managers, deployment servers, search head cluster deployers) are indexing their own events locally, instead of forwarding the data to the indexers, per best practices.
failure_text = One or more non-indexer instances is not forwarding their events to the indexers. This can isolate some of your data and prevent some Monitoring Console dashboards from working.
suggested_action = Make sure that all non-indexer instances are forwarding their internal events to the indexers, per best practices.
doc_link = healthcheck.internallogs.sh, healthcheck.internallogs.cm
doc_title = indexing on search heads, indexing on cluster managers
applicable_to_groups =
environments_to_exclude = standalone
search = | rest $rest_scope$ /services/server/info \
| fields splunk_server isForwarding server_roles \
| where (server_roles=="search_head" OR server_roles=="cluster_master" OR server_roles=="license_master" OR server_roles=="shc_deployer" OR server_roles=="deployment_server" OR isNull(server_roles)) \
| eval severity_level = case( isForwarding==1, 0, isForwarding == 0, 2, true(), -1) \
| rename splunk_server AS instance

[near_critical_disk_usage]
title = Near-critical disk usage
category = System and Environment
tags = capacity, storage, searches_skipped, disk_space
description = This checks the disk usage of partitions that Splunk Enterprise reads or writes to.
failure_text = At least one partition, on at least one instance, was found with disk usage above 90%.
suggested_action = Check the data retention policies on the affected instances to ensure that the disk doesn't fill up. Alternatively, provision more disk space. You can also enable a platform alert for this condition. Go to the Monitoring Console's alert setup page and enable the "Near-critical Disk Usage" platform alert.
doc_link = healthcheck.disk.usage.indexer, healthcheck.disk.usage.searchhead
doc_title = disk usage on indexers, disk usage on search heads
applicable_to_groups =
search = | rest $rest_scope$ /services/server/status/partitions-space \
| eval free = if(isnotnull(available), available, free) \
| eval usage = capacity - free \
| eval pct_usage = floor(usage / capacity * 100) \
| stats first(pct_usage) AS pct_usage by splunk_server, mount_point \
| eval severity_level = case(pct_usage < 90, 0, pct_usage >= 90, 2, True(), -1) \
| eval mount_info = if(isNull(pct_usage), "No disk information available", mount_point.": ".pct_usage."%") \
| stats values(mount_info) AS mount_information max(severity_level) AS severity_level by splunk_server \
| lookup local=true dmc_assets serverName AS splunk_server OUTPUT machine \
| eval machine = mvdedup(machine) \
| fields splunk_server machine mount_information severity_level \
| rename splunk_server AS instance
drilldown = /app/splunk_monitoring_console/resource_usage_machine?form.machine=$machine$

[saturation_of_event_processing_queues]
title = Saturation of event-processing queues
category = Data Indexing
tags = indexing, queues
description = This checks if queues on your indexers have been saturated (i.e: 90% full or more) consistently in the past 15 minutes.
failure_text = One or more of the indexer queues on this instance is reporting an averaged fill percentage of 90 or more over the last 15 minutes. This might affect how quickly this instance indexes events.
suggested_action = Check the Monitoring Console indexing performance view for the affected indexers.
doc_link = healthcheck.indexer.highqueuefillpct
doc_title = saturation of event processing queues
applicable_to_groups = dmc_group_indexer
search = | rest $rest_scope$ /services/server/introspection/queues \
| search title=parsingQueue* OR title=aggQueue* OR title=typingQueue* OR title=indexQueue* \
| eval fill_perc = round(current_size_bytes / max_size_bytes * 100,2) \
| rex field=title "(?<queue_name>^\w+)(?:\.(?<pipeline_number>\d+))?" \
| fields splunk_server fill_perc queue_name pipeline_number \
| eval severity_level = if( fill_perc > 90, 2, 0) \
| eval fill_perc = if(isnotnull(pipeline_number), "pset".pipeline_number.": ".fill_perc, fill_perc) \
| chart values(fill_perc) AS fill_perc max(severity_level) AS severity_level over splunk_server by queue_name \
| rename "fill_perc: *" AS * "severity_level: *" AS "sev_*" \
| eval severity_level = max(sev_parsingQueue, sev_aggQueue, sev_typingQueue, sev_indexQueue) \
| fields splunk_server severity_level parsingQueue aggQueue typingQueue indexQueue \
| rename splunk_server AS instance parsingQueue as "Parsing Queue Fill Ratio (%)", aggQueue as "Aggregation Queue Fill Ratio (%)", "typingQueue" as "Typing Queue Fill Ratio (%)", indexQueue as "Indexing Queue Fill Ratio (%)"
drilldown = /app/splunk_monitoring_console/indexing_performance_instance?form.splunk_server=$instance$

[indexing_status]
title = Indexing status
category = Data Indexing
tags = indexing, buckets
description = This tests the current status of the indexer processor on indexer instances.
failure_text = One or more of your indexers reports an abnormal state. Those indexers might be blocked or causing delays in data acquisition.
suggested_action = Investigate affected indexers with the Monitoring Console Indexing Performance: Instance view. You might need to restart the affected indexers.
doc_link = healthcheck.indexing.status
doc_title = Splunk indexing status
applicable_to_groups = dmc_group_indexer
search = | rest $rest_scope$ splunk_server_group=dmc_group_indexer /services/server/introspection/indexer \
| fields splunk_server, status, reason \
| eval severity_level = if(status == "normal", 0, 2) \
| rename splunk_server as instance
drilldown = /app/splunk_monitoring_console/indexing_performance_instance?form.splunk_server=$instance$

[excessive_physical_memory_usage]
title = Excessive physical memory usage
category = Splunk Miscellaneous
tags = resource_usage
description = This check assesses system-wide physical memory usage and raises a warning for those servers where it is > 90%.
failure_text = One or more machines are using more than 90% of installed physical memory and may be at risk of an outage.
suggested_action = Check individual machines to identify the source of the excessive memory usage with the Monitoring Console Resource Usage: Machine or Resource Usage: Instance pages.
doc_link = healthcheck.hardware.highmem
doc_title = high memory usage
applicable_to_groups =
search = | rest $rest_scope$ /services/server/status/resource-usage/hostwide \
| eval percentage = round(mem_used/mem,3)*100 \
| eval severity_level = if(percentage >= 90, 2, 0) \
| lookup local=true dmc_assets serverName AS splunk_server OUTPUT machine \
| eval machine = mvdedup(machine) \
| fields splunk_server, machine, mem_used, mem, percentage, severity_level \
| rename splunk_server AS instance, mem AS "Physical memory installed (MB)", percentage AS "Memory used (%)", mem_used AS "Memory used (MB)"
drilldown = /app/splunk_monitoring_console/resource_usage_machine?form.machine=$machine$

[integrity_check_installed_files]
title = Integrity check of installed files
category = Splunk Miscellaneous
tags = configuration, installation
description = Verifies the integrity of files that were installed by Splunk.
failure_text = One or more files that were originally installed by Splunk have been unexpectedly modified or are missing.
suggested_action = Review the list of files that failed the integrity check. Make sure that no default configuration files have been modified. Consider restoring non-configuration files from backup or from the Splunk package.
doc_link = healthcheck.validate.files
doc_title = validating installed files
applicable_to_groups =
search = | rest $rest_scope$ /services/server/info \
| join type=outer splunk_server [rest $rest_scope$ /services/server/status/installed-file-integrity \
    | fields splunk_server check_ready check_failures.* \
    | untable splunk_server file_path check_result \
    | replace "check_failures.*" WITH "*" IN file_path \
    | eval check_ready = if(file_path == "check_ready", check_result, NULL) \
    | eval file_path = if(file_path == "check_ready", NULL, file_path) \
    | eval checker_not_working = if(file_path == "fail", check_result, NULL) \
    | stats count(eval(isnotnull(file_path))) AS file_integrity_failures max(check_ready) AS check_ready first(checker_not_working) AS checker_not_working by splunk_server] \
| fields splunk_server check_ready file_integrity_failures checker_not_working \
| eval severity_level = case(check_ready != "true", -1, file_integrity_failures = 0, 0, isnull(file_integrity_failures), 1, checker_not_working == "check_disabled", 1, file_integrity_failures > 0, 2, True(), -1) \
| eval file_integrity_failures = if(isnotnull(checker_not_working), checker_not_working, file_integrity_failures) \
| eval file_integrity_failures = if(isnotnull(file_integrity_failures), file_integrity_failures, "integrity check not available") \
| eval check_ready = if(isnotnull(check_ready), check_ready, "integrity check not available") \
| rename splunk_server AS instance \
| fields - checker_not_working _timediff
drilldown = /app/search/integrity_check_of_installed_files?form.splunk_server=$instance$

[event_processing_issues]
title = Event-processing issues
category = Data Collection
tags = event_breaking, indexing, timestamp_extraction
description = This checks for warnings emitted when the index-time settings in place do not allow for the proper processing of recently ingested data (in the last hour).
failure_text = Some recently ingested events are triggering event-processing warnings and indicate the presence of one or more of these scenarios:  \
 1. Lines in the event are too long, exceeding props.conf / TRUNCATE  \
 2. There are too many lines per event, exceeding props.conf / MAX_EVENTS  \
 3. The extraction of event time stamps was partially or completely unsuccessful  \
These event-processing issues can have a negative impact on the performance of data ingestion.
suggested_action = Check the events that are triggering these warnings. Adjust event-processing settings as needed to ensure their proper ingestion.
doc_link = healthcheck.eventprocess.linebreak, healthcheck.eventprocess.timestamp
doc_title = event processing issues caused by line breaks, event processing issues caused by timestamps
applicable_to_groups =
search = `dmc_set_index_internal` $rest_scope$ search_group=dmc_group_indexer earliest=-60m (source=*splunkd.log* (component=AggregatorMiningProcessor OR component= LineBreakingProcessor OR component=DateParserVerbose) (log_level=WARN OR log_level=ERROR)) OR (source=*metrics.log* group=thruput name=index_thruput) \
| stats sum(eval(round(ev,0))) AS event_count count(eval(component=="AggregatorMiningProcessor")) AS aggregation_issues count(eval(component=="LineBreakingProcessor")) AS line_breaking_issues count(eval(component=="DateParserVerbose")) AS date_parsing_issues by host \
| eval crap_score = round((aggregation_issues + line_breaking_issues + date_parsing_issues) / event_count * 1000, 3) \
| eval severity_level = case(crap_score == 0, 0, crap_score > 0 AND crap_score < 1, 1, True(), 2) \
| rename host AS instance \
| fields - crap_score
drilldown = `dmc_set_index_internal` host=$instance$ earliest=-60m source=*splunkd.log* (component=AggregatorMiningProcessor OR component= LineBreakingProcessor OR component=DateParserVerbose) (log_level=WARN OR log_level=ERROR) \
| rex field=event_message "Context: source(::|=)(?<context_source>[^\|]*?)\|host(::|=)(?<context_host>[^\|]*?)\|(?<context_sourcetype>[^\|]*?)\|" \
| eval data_source = if(isnull(data_source) AND isnotnull(context_source), context_source, data_source) \
| eval data_host = if(isnull(data_host) AND isnotnull(context_host), context_host, data_host) \
| eval data_sourcetype = if(isnull(data_sourcetype) AND isnotnull(context_sourcetype), context_sourcetype, data_sourcetype) \
| stats count AS processing_errors count(eval(component=="AggregatorMiningProcessor")) AS aggregation_issues count(eval(component=="LineBreakingProcessor")) AS line_breaking_issues count(eval(component=="DateParserVerbose")) AS date_parsing_issues dc(data_host) AS host_count dc(data_source) AS source_count by data_sourcetype \
| sort - processing_errors

[kv_store_status]
title = KV Store status
category = Splunk Miscellaneous
tags = kv_store
description = This checks whether the KV Store is running as expected.
failure_text = The status of the KV Store feature on one or more instances is abnormal.
suggested_action = Check mongod.log to find out why the KV Store process may have failed to start.
doc_link = healthcheck.kvstore.status
doc_title = KV store statuses
applicable_to_groups = dmc_group_kv_store
search = | rest $rest_scope$ splunk_server_group=dmc_group_kv_store /services/server/info \
| fields splunk_server kvStoreStatus \
| eval severity_level = case(isNull(kvStoreStatus), -1, kvStoreStatus == "ready", 0, kvStoreStatus == "disabled" OR kvStoreStatus == "starting", 1, kvStoreStatus == "failed", 2) \
| rename splunk_server AS instance
drilldown = /app/splunk_monitoring_console/kv_store_instance?form.splunk_server=$instance$
